{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add the project root directory to the Python path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import project modules\n",
    "from src.data.data_processor import NetworkDataProcessor\n",
    "from src.models.anomaly_detector import AnomalyDetector, EnsembleAnomalyDetector\n",
    "from src.utils.behavioral_profiling import EntityProfiler\n",
    "from src.config import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory if it doesn't exist\n",
    "os.makedirs('../data', exist_ok=True)\n",
    "\n",
    "# Check if the dataset is already downloaded\n",
    "dataset_path = '../data/synthetic_network_traffic.csv'\n",
    "\n",
    "if not os.path.exists(dataset_path):\n",
    "    print(\"Dataset not found. Generating synthetic dataset...\")\n",
    "    from src.data.download_dataset import generate_synthetic_dataset\n",
    "    dataset_path = generate_synthetic_dataset()\n",
    "else:\n",
    "    print(f\"Dataset found at {dataset_path}\")\n",
    "\n",
    "# Initialize data processor\n",
    "processor = NetworkDataProcessor(scaling_method='standard', handle_imbalance=True)\n",
    "\n",
    "# Load data\n",
    "df = processor.load_data(dataset_path)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "display(df.head())\n",
    "\n",
    "# Check class distribution\n",
    "if 'Label' in df.columns:\n",
    "    print(\"\\nClass distribution:\")\n",
    "    display(df['Label'].value_counts())\n",
    "    \n",
    "    # Plot class distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(x='Label', data=df)\n",
    "    plt.title('Class Distribution')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify column types\n",
    "processor.target_column = 'Label'\n",
    "numerical_columns, categorical_columns = processor.identify_column_types(df)\n",
    "\n",
    "# Handle missing values\n",
    "df = processor.handle_missing_values(df)\n",
    "\n",
    "# Display information about numerical and categorical features\n",
    "print(f\"Numerical features ({len(numerical_columns)}):\")\n",
    "print(numerical_columns[:10], \"...\" if len(numerical_columns) > 10 else \"\")\n",
    "print(f\"\\nCategorical features ({len(categorical_columns)}):\")\n",
    "print(categorical_columns)\n",
    "\n",
    "# Preprocess data\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = processor.preprocess_data(\n",
    "    df, \n",
    "    categorical_encoding='onehot',\n",
    "    handle_imbalance_method='smote'\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "\n",
    "# Check class distribution after preprocessing\n",
    "print(\"\\nClass distribution after preprocessing:\")\n",
    "print(f\"Training set: {pd.Series(y_train).value_counts().to_dict()}\")\n",
    "print(f\"Validation set: {pd.Series(y_val).value_counts().to_dict()}\")\n",
    "print(f\"Test set: {pd.Series(y_test).value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results directory\n",
    "results_dir = '../results/notebook_demo'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Train and evaluate Isolation Forest\n",
    "print(\"\\n=== Isolation Forest ===\\n\")\n",
    "isolation_forest = AnomalyDetector(model_type='isolation_forest', contamination=0.05)\n",
    "isolation_forest.fit(X_train)\n",
    "isolation_forest.find_optimal_threshold(X_val, y_val, metric='f1')\n",
    "isolation_forest_metrics = isolation_forest.evaluate(X_test, y_test)\n",
    "\n",
    "# Train and evaluate One-Class SVM\n",
    "print(\"\\n=== One-Class SVM ===\\n\")\n",
    "one_class_svm = AnomalyDetector(model_type='one_class_svm')\n",
    "one_class_svm.fit(X_train)\n",
    "one_class_svm.find_optimal_threshold(X_val, y_val, metric='f1')\n",
    "one_class_svm_metrics = one_class_svm.evaluate(X_test, y_test)\n",
    "\n",
    "# Train and evaluate Local Outlier Factor\n",
    "print(\"\\n=== Local Outlier Factor ===\\n\")\n",
    "lof = AnomalyDetector(model_type='local_outlier_factor', contamination=0.05)\n",
    "lof.fit(X_train)\n",
    "lof.find_optimal_threshold(X_val, y_val, metric='f1')\n",
    "lof_metrics = lof.evaluate(X_test, y_test)\n",
    "\n",
    "# Compare model performance\n",
    "metrics_df = pd.DataFrame([\n",
    "    {'Model': 'Isolation Forest', **isolation_forest_metrics},\n",
    "    {'Model': 'One-Class SVM', **one_class_svm_metrics},\n",
    "    {'Model': 'Local Outlier Factor', **lof_metrics}\n",
    "])\n",
    "\n",
    "display(metrics_df)\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "metrics_to_plot = ['precision', 'recall', 'f1', 'false_positive_rate']\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    plt.subplot(1, len(metrics_to_plot), i+1)\n",
    "    sns.barplot(x='Model', y=metric, data=metrics_df)\n",
    "    plt.title(metric.capitalize())\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ensemble of models\n",
    "print(\"\\n=== Ensemble Model ===\\n\")\n",
    "ensemble = EnsembleAnomalyDetector(voting='soft')\n",
    "\n",
    "# Add individual models to the ensemble\n",
    "ensemble.add_model(isolation_forest)\n",
    "ensemble.add_model(one_class_svm)\n",
    "ensemble.add_model(lof)\n",
    "\n",
    "# Evaluate the ensemble\n",
    "ensemble.find_optimal_threshold(X_val, y_val, metric='f1')\n",
    "ensemble_metrics = ensemble.evaluate(X_test, y_test)\n",
    "\n",
    "# Add ensemble metrics to the comparison\n",
    "metrics_df = pd.concat([\n",
    "    metrics_df,\n",
    "    pd.DataFrame([{'Model': 'Ensemble', **ensemble_metrics}])\n",
    "])\n",
    "\n",
    "display(metrics_df)\n",
    "\n",
    "# Plot updated comparison\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    plt.subplot(1, len(metrics_to_plot), i+1)\n",
    "    sns.barplot(x='Model', y=metric, data=metrics_df)\n",
    "    plt.title(metric.capitalize())\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize entity profiler\n",
    "print(\"\\n=== Behavioral Profiling ===\\n\")\n",
    "profiler = EntityProfiler(entity_column='Source IP', time_window=300)\n",
    "\n",
    "# Create entity features\n",
    "timestamp_column = 'Timestamp' if 'Timestamp' in df.columns else None\n",
    "\n",
    "if timestamp_column:\n",
    "    # Create entity features\n",
    "    entity_features = profiler.create_entity_features(df, timestamp_column)\n",
    "    \n",
    "    # Display entity features\n",
    "    print(\"Entity features:\")\n",
    "    display(entity_features.head())\n",
    "    \n",
    "    # Build entity profiles\n",
    "    entity_profiles = profiler.build_entity_profiles(entity_features)\n",
    "    \n",
    "    # Detect anomalous entities\n",
    "    anomalous_entities = profiler.detect_anomalous_entities(threshold=2.0)\n",
    "    \n",
    "    # Display anomalous entities\n",
    "    print(f\"\\nDetected {len(anomalous_entities)} anomalous entities\")\n",
    "    \n",
    "    if anomalous_entities:\n",
    "        anomalies_df = pd.DataFrame([\n",
    "            {\n",
    "                'entity': entity,\n",
    "                'anomaly_score': data['anomaly_score'],\n",
    "                'num_anomalous_features': data['num_anomalous_features'],\n",
    "                'total_features': data['total_features']\n",
    "            }\n",
    "            for entity, data in anomalous_entities.items()\n",
    "        ])\n",
    "        \n",
    "        display(anomalies_df.head(10))\n",
    "        \n",
    "        # Plot anomaly scores\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.barplot(x='entity', y='anomaly_score', data=anomalies_df.head(10))\n",
    "        plt.title('Top 10 Anomalous Entities')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No timestamp column found in the dataset. Skipping behavioral profiling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If behavioral profiling was successful, cluster the entities\n",
    "if 'entity_features' in locals() and 'profiler' in locals():\n",
    "    # Cluster entities\n",
    "    print(\"\\n=== Entity Clustering ===\\n\")\n",
    "    clustered_entities = profiler.cluster_entities(n_clusters=5, method='kmeans')\n",
    "    \n",
    "    # Display cluster information\n",
    "    cluster_counts = clustered_entities['cluster'].value_counts().sort_index()\n",
    "    print(\"Entities per cluster:\")\n",
    "    display(cluster_counts)\n",
    "    \n",
    "    # Visualize entity clusters\n",
    "    profiler.visualize_entity_clusters(clustered_entities)\n",
    "    \n",
    "    # If there are anomalous entities, track one of them over time\n",
    "    if anomalous_entities:\n",
    "        # Get the most anomalous entity\n",
    "        most_anomalous = list(anomalous_entities.keys())[0]\n",
    "        \n",
    "        # Track its behavior over time\n",
    "        print(f\"\\n=== Tracking Behavior of Entity {most_anomalous} ===\\n\")\n",
    "        entity_data = profiler.track_entity_behavior_over_time(entity_features, most_anomalous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if both anomaly detection and behavioral profiling were performed\n",
    "if 'ensemble_metrics' in locals() and 'anomalous_entities' in locals() and anomalous_entities:\n",
    "    print(\"\\n=== Combining Anomaly Detection and Behavioral Profiling ===\\n\")\n",
    "    \n",
    "    # Get anomaly scores from the ensemble model\n",
    "    anomaly_scores = ensemble.decision_function(X_test)\n",
    "    \n",
    "    # Create a dataframe with test data and anomaly scores\n",
    "    test_df = X_test.copy()\n",
    "    test_df['true_label'] = y_test.values\n",
    "    test_df['anomaly_score'] = anomaly_scores\n",
    "    test_df['predicted_label'] = ensemble.predict(X_test)\n",
    "    \n",
    "    # If 'Source IP' is in the test data, we can combine with behavioral profiling\n",
    "    if 'Source IP' in test_df.columns:\n",
    "        # Get the list of anomalous entities\n",
    "        anomalous_ips = list(anomalous_entities.keys())\n",
    "        \n",
    "        # Mark rows with anomalous entities\n",
    "        test_df['entity_anomalous'] = test_df['Source IP'].isin(anomalous_ips)\n",
    "        \n",
    "        # Combine predictions: flag as anomaly if both model and behavioral profiling agree\n",
    "        test_df['combined_prediction'] = ((test_df['predicted_label'] == -1) & test_df['entity_anomalous']).astype(int)\n",
    "        test_df['combined_prediction'] = test_df['combined_prediction'].replace({0: 1, 1: -1})  # Convert to 1/-1 format\n",
    "        \n",
    "        # Calculate metrics for the combined approach\n",
    "        from sklearn.metrics import confusion_matrix, classification_report\n",
    "        \n",
    "        # Convert true labels to match model output (1 for normal, -1 for anomaly)\n",
    "        y_test_converted = np.where(y_test == 0, -1, 1)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        combined_cm = confusion_matrix(y_test_converted, test_df['combined_prediction'])\n",
    "        combined_report = classification_report(y_test_converted, test_df['combined_prediction'], output_dict=True)\n",
    "        \n",
    "        # Calculate false positive rate\n",
    "        tn, fp, fn, tp = combined_cm.ravel()\n",
    "        combined_fpr = fp / (fp + tn)\n",
    "        \n",
    "        # Compile metrics\n",
    "        combined_metrics = {\n",
    "            'accuracy': combined_report['accuracy'],\n",
    "            'precision': combined_report['-1']['precision'],  # Precision for anomaly class\n",
    "            'recall': combined_report['-1']['recall'],  # Recall for anomaly class\n",
    "            'f1': combined_report['-1']['f1-score'],  # F1 for anomaly class\n",
    "            'false_positive_rate': combined_fpr\n",
    "        }\n",
    "        \n",
    "        # Add combined metrics to the comparison\n",
    "        metrics_df = pd.concat([\n",
    "            metrics_df,\n",
    "            pd.DataFrame([{'Model': 'Combined Approach', **combined_metrics}])\n",
    "        ])\n",
    "        \n",
    "        display(metrics_df)\n",
    "        \n",
    "        # Plot updated comparison\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        \n",
    "        for i, metric in enumerate(metrics_to_plot):\n",
    "            plt.subplot(1, len(metrics_to_plot), i+1)\n",
    "            sns.barplot(x='Model', y=metric, data=metrics_df)\n",
    "            plt.title(metric.capitalize())\n",
    "            plt.ylim(0, 1)\n",
    "            plt.xticks(rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot confusion matrix for the combined approach\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(combined_cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=['Normal', 'Anomaly'], \n",
    "                    yticklabels=['Normal', 'Anomaly'])\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title('Confusion Matrix - Combined Approach')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"'Source IP' column not found in the test data. Cannot combine with behavioral profiling.\")\n",
    "else:\n",
    "    print(\"Either anomaly detection or behavioral profiling was not performed. Cannot combine approaches.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "if 'ensemble' in locals():\n",
    "    model_path = os.path.join(results_dir, 'ensemble_model.joblib')\n",
    "    ensemble.save_model(model_path)\n",
    "    print(f\"Ensemble model saved to {model_path}\")\n",
    "\n",
    "# Save the entity profiles if available\n",
    "if 'profiler' in locals() and hasattr(profiler, 'entity_profiles') and profiler.entity_profiles:\n",
    "    profile_path = os.path.join(results_dir, 'entity_profiles.csv')\n",
    "    profiler.save_profiles(profile_path)\n",
    "    print(f\"Entity profiles saved to {profile_path}\")\n",
    "\n",
    "print(\"\\nNotebook execution completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
